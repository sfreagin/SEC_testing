{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to scrape SEC.gov\n",
    "\n",
    "I'm going to follow along with this video series from SigmaCoding on YouTube: https://youtu.be/TxUmufNnIaA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to be used later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_windows_1252_characters(restore_string):\n",
    "    \n",
    "    # this replaces the C1 characters in the Unicode string s\n",
    "    # it it ultiamtely text normalization\n",
    "    \n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "        except UnicodeDecodeError:\n",
    "            # no character at the corresponding code point: remove it\n",
    "            return ''\n",
    "        \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab the document content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define URL to the html_text file\n",
    "new_html_text = r'https://www.sec.gov/Archives/edgar/data/1166036/000110465904027382/0001104659-04-027382.txt'\n",
    "\n",
    "# https://www.sec.gov/Archives/edgar/data/1166036/000110465904027382/0001104659-04-027382.txt\n",
    "# https://www.sec.gov/Archives/edgar/data/50863/000119312521009142/0001193125-21-009142.txt\n",
    "# https://www.sec.gov/Archives/edgar/data/21344/000155278121000188/0001552781-21-000188.txt\n",
    "################### 8-K example links ^^^^\n",
    "\n",
    "# grab the response\n",
    "response = requests.get(new_html_text)\n",
    "\n",
    "# parse the response\n",
    "soup = BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define master dictionary for all filings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a new dictionary that will house all our filings\n",
    "master_filings_dict = {}\n",
    "\n",
    "#define unique key for each filing\n",
    "#for now, each number is identical to its above .txt slug\n",
    "accession_number = '000110465904027382/0001104659-04-027382'\n",
    "\n",
    "# add the key to the dictionary and add a new 'level'(?)\n",
    "master_filings_dict[accession_number] = {}\n",
    "master_filings_dict[accession_number]['sec_header_content'] = {}\n",
    "master_filings_dict[accession_number]['filing_documents'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the SEC-Header Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sec-header>0001104659-04-027382.hdr.sgml : 20040913\n",
       "<acceptance-datetime>20040913074905\n",
       "ACCESSION NUMBER:\t\t0001104659-04-027382\n",
       "CONFORMED SUBMISSION TYPE:\t8-K/A\n",
       "PUBLIC DOCUMENT COUNT:\t\t7\n",
       "CONFORMED PERIOD OF REPORT:\t20040730\n",
       "ITEM INFORMATION:\t\tCompletion of Acquisition or Disposition of Assets\n",
       "ITEM INFORMATION:\t\tFinancial Statements and Exhibits\n",
       "FILED AS OF DATE:\t\t20040913\n",
       "DATE AS OF CHANGE:\t\t20040913\n",
       "\n",
       "FILER:\n",
       "\n",
       "\tCOMPANY DATA:\t\n",
       "\t\tCOMPANY CONFORMED NAME:\t\t\tMARKWEST ENERGY PARTNERS L P\n",
       "\t\tCENTRAL INDEX KEY:\t\t\t0001166036\n",
       "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tCRUDE PETROLEUM &amp; NATURAL GAS [1311]\n",
       "\t\tIRS NUMBER:\t\t\t\t270005456\n",
       "\t\tFISCAL YEAR END:\t\t\t1231\n",
       "\n",
       "\tFILING VALUES:\n",
       "\t\tFORM TYPE:\t\t8-K/A\n",
       "\t\tSEC ACT:\t\t1934 Act\n",
       "\t\tSEC FILE NUMBER:\t001-31239\n",
       "\t\tFILM NUMBER:\t\t041026639\n",
       "\n",
       "\tBUSINESS ADDRESS:\t\n",
       "\t\tSTREET 1:\t\t155 INVERNESS DR WEST\n",
       "\t\tSTREET 2:\t\tSTE 200\n",
       "\t\tCITY:\t\t\tENGLEWOOD\n",
       "\t\tSTATE:\t\t\tCO\n",
       "\t\tZIP:\t\t\t80112\n",
       "\t\tBUSINESS PHONE:\t\t303-925-9275\n",
       "\n",
       "\tMAIL ADDRESS:\t\n",
       "\t\tSTREET 1:\t\t155 INVERNESS DR WEST\n",
       "\t\tSTREET 2:\t\tSTE 200\n",
       "\t\tCITY:\t\t\tENGLEWOOD\n",
       "\t\tSTATE:\t\t\tCO\n",
       "\t\tZIP:\t\t\t80112\n",
       "</acceptance-datetime></sec-header>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grab the sec-header document\n",
    "sec_header_tag = soup.find('sec-header')\n",
    "'''# check out it:\n",
    "print(sec_header_tag.get_text())\n",
    "# and please recomment, thank you!\n",
    "'''\n",
    "#store the sec header content inside the dictionary\n",
    "master_filings_dict[accession_number]['sec_header_content']['sec_header_code'] = sec_header_tag\n",
    "\n",
    "display(sec_header_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "The document 8-K/A was parsed.\n",
      "There was 31 page(s) found.\n",
      "There was 31 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-2.1 was parsed.\n",
      "There was 37 page(s) found.\n",
      "There was 37 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-4.1 was parsed.\n",
      "There was 35 page(s) found.\n",
      "There was 35 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-4.2 was parsed.\n",
      "There was 20 page(s) found.\n",
      "There was 20 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-23.1 was parsed.\n",
      "There was 1 page(s) found.\n",
      "There was 0 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-99.1 was parsed.\n",
      "There was 235 page(s) found.\n",
      "There was 235 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-99.2 was parsed.\n",
      "There was 13 page(s) found.\n",
      "There was 13 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 000110465904027382/0001104659-04-027382 were parsed and stored.\n"
     ]
    }
   ],
   "source": [
    "# initalize the dictionary that will house all of our documents\n",
    "master_document_dict = {}\n",
    "\n",
    "# find all the documents in the filing.\n",
    "for filing_document in soup.find_all('document'):\n",
    "    \n",
    "    # define the document type, found under the <type> tag, this will serve as our key for the dictionary.\n",
    "    document_id = filing_document.type.find(text=True, recursive=False).strip()\n",
    "    \n",
    "    # here are the other parts if you want them.\n",
    "    document_sequence = filing_document.sequence.find(text=True, recursive=False).strip()\n",
    "    document_filename = filing_document.filename.find(text=True, recursive=False).strip()\n",
    "    document_description = filing_document.description.find(text=True, recursive=False).strip()\n",
    "    \n",
    "    # initalize our document dictionary\n",
    "    master_document_dict[document_id] = {}\n",
    "    \n",
    "    # add the different parts, we parsed up above.\n",
    "    master_document_dict[document_id]['document_sequence'] = document_sequence\n",
    "    master_document_dict[document_id]['document_filename'] = document_filename\n",
    "    master_document_dict[document_id]['document_description'] = document_description\n",
    "    \n",
    "    # store the document itself, this portion extracts the HTML code. We will have to reparse it later.\n",
    "    master_document_dict[document_id]['document_code'] = filing_document.extract()\n",
    "    \n",
    "    \n",
    "    # grab the text portion of the document, this will be used to split the document into pages.\n",
    "    filing_doc_text = filing_document.find('text').extract()\n",
    "\n",
    "    \n",
    "    # find all the thematic breaks, these help define page numbers and page breaks.\n",
    "    all_thematic_breaks = filing_doc_text.find_all('hr',{'width':'100%'})\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        THE FOLLOWING CODE IS OPTIONAL:\n",
    "        -------------------------------\n",
    "        \n",
    "        This portion will demonstrate how to parse the page number from each \"page\". Now I would only do this if you\n",
    "        want the ACTUAL page number on the document, if you don't need it then forget about it and just wait till the\n",
    "        next seciton.\n",
    "        \n",
    "        Additionally, some of the documents appear not to have page numbers when they should so there is no guarantee\n",
    "        that all the documents will be nice and organized.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # grab all the page numbers, first one is usually blank\n",
    "    all_page_numbers = [thematic_break.parent.parent.previous_sibling.previous_sibling.get_text(strip=True) \n",
    "                        for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "        If the above list comprehension doesn't make sense to you, here is how it would look as a regular loop.\n",
    "    \n",
    "        # define a list to house all the page numbers\n",
    "        all_page_numbers = []\n",
    "\n",
    "        # loop throuhg all the thematic breaks.\n",
    "        for thematic_break in all thematic_breaks:\n",
    "\n",
    "           # this would grab the page number tag.\n",
    "           page_number = thematic_break.parent.parent.previous_sibling.previous_sibling\n",
    "\n",
    "           # this would grab the page number text\n",
    "           page_number = page_number.get_text(strip=True)\n",
    "           \n",
    "           # store it in the list.\n",
    "           all_page_numbers.append(page_number)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # determine the number of pages, will be used for the upcoming if conditions.\n",
    "    length_of_page_numbers = len(all_page_numbers)\n",
    "    \n",
    "    # as long as there are numbers to change then proceed.\n",
    "    if length_of_page_numbers > 0:\n",
    "        \n",
    "        # grab the last number\n",
    "        previous_number = all_page_numbers[-1]\n",
    "        \n",
    "        # initalize a new list\n",
    "        all_page_numbers_cleaned = []\n",
    "        \n",
    "        # loop through the old list in reverse order.\n",
    "        for number in reversed(all_page_numbers):\n",
    "            \n",
    "            # if it's blank proceed to cleaning.\n",
    "            if number == '':\n",
    "                \n",
    "                # the tricky part, there are three scenarios.\n",
    "\n",
    "                # the previous one we looped was 0 or 1.\n",
    "                if previous_number == '1' or previous_number == '0':\n",
    "                    \n",
    "                    # in this case, it means this is a \"new section\", so restart at 0.\n",
    "                    all_page_numbers_cleaned.append(str(0))\n",
    "                    \n",
    "                    # reset the page number and the previous number.\n",
    "                    length_of_page_numbers = length_of_page_numbers - 1\n",
    "                    previous_number = '0'\n",
    "                \n",
    "                # the previous one we looped it wasn't either of those.\n",
    "                else:\n",
    "                    \n",
    "                    # if it was blank, take the current length, subtract 1, and add it to the list.\n",
    "                    all_page_numbers_cleaned.append(str(length_of_page_numbers - 1))\n",
    "                    \n",
    "                    # reset the page number and the previous number.\n",
    "                    length_of_page_numbers = length_of_page_numbers - 1\n",
    "                    previous_number = number\n",
    "\n",
    "            else:\n",
    "                \n",
    "                # add the number to the list.\n",
    "                all_page_numbers_cleaned.append(number)\n",
    "                \n",
    "                # reset the page number and the previous number.\n",
    "                length_of_page_numbers = length_of_page_numbers - 1\n",
    "                previous_number = number\n",
    "    else:\n",
    "        \n",
    "        # make sure that it has a page number even if there are none, just have it equal 0\n",
    "        all_page_numbers_cleaned = ['0']\n",
    "    \n",
    "    # have the page numbers be the cleaned ones, in reversed order.\n",
    "    all_page_numbers = list(reversed(all_page_numbers_cleaned))\n",
    "    \n",
    "    # store the page_numbers\n",
    "    master_document_dict[document_id]['page_numbers'] = all_page_numbers\n",
    "    \n",
    "    \n",
    "    '''\n",
    "        -------------------------------\n",
    "          THE OPTIONAL CODE HAS ENDED\n",
    "        -------------------------------\n",
    "    \n",
    "        This next portion of code is really what made this all possible. Up above you saw I grabbed all the thematic\n",
    "        breaks from our document because they sever as natural page breaks. Without those thematic breaks I'm not sure\n",
    "        if this would be such an easy process. It's not to say we couldn't break it into pages, but I would bet the code\n",
    "        would be more complex.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # convert all thematic breaks to a string so it can be used for parsing\n",
    "    all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    # prep the document text for splitting, this means converting it to a string.\n",
    "    filing_doc_string = str(filing_doc_text)\n",
    "\n",
    "    \n",
    "    # handle the case where there are thematic breaks.\n",
    "    if len(all_thematic_breaks) > 0:\n",
    "    \n",
    "        # define the regex delimiter pattern, this would just be all of our thematic breaks.\n",
    "        regex_delimiter_pattern = '|'.join(map(re.escape, all_thematic_breaks))\n",
    "\n",
    "        # split the document along each thematic break.\n",
    "        split_filing_string = re.split(regex_delimiter_pattern, filing_doc_string)\n",
    "\n",
    "        # store the document itself\n",
    "        master_document_dict[document_id]['pages_code'] = split_filing_string\n",
    "\n",
    "    # handle the case where there are no thematic breaks.\n",
    "    elif len(all_thematic_breaks) == 0:\n",
    "\n",
    "        # handles so it will display correctly.\n",
    "        split_filing_string = all_thematic_breaks\n",
    "        \n",
    "        # store the document as is, since there are no thematic breaks. In other words, no splitting.\n",
    "        master_document_dict[document_id]['pages_code'] = [filing_doc_string]\n",
    "    \n",
    "\n",
    "    # display some information to the user.\n",
    "    print('-'*80)\n",
    "    print('The document {} was parsed.'.format(document_id))\n",
    "    print('There was {} page(s) found.'.format(len(all_page_numbers)))\n",
    "    print('There was {} thematic breaks(s) found.'.format(len(all_thematic_breaks)))\n",
    "    \n",
    "\n",
    "# store the documents in the master_filing_dictionary.\n",
    "master_filings_dict[accession_number]['filing_documents'] = master_document_dict\n",
    "\n",
    "print('-'*80)\n",
    "print('All the documents for filing {} were parsed and stored.'.format(accession_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 000110465904027382/0001104659-04-027382 were parsed and stored.\n"
     ]
    }
   ],
   "source": [
    "# initialize master document dictionary\n",
    "master_document_dict = {}\n",
    "\n",
    "# loop through each doc in filing\n",
    "for filing_document in soup.find_all('document'):\n",
    "    \n",
    "    #define document id\n",
    "    document_id = filing_document.type.find(text=True, recursive = False).strip()\n",
    "    \n",
    "    ## try it out:\n",
    "    #print(document_id)\n",
    "    ## and please recomment, thank you!\n",
    "\n",
    "    # document sequence\n",
    "    document_sequence = filing_document.sequence.find(text = True, recursive = False).strip()\n",
    "    \n",
    "    # document filename\n",
    "    document_filename = filing_document.filename.find(text = True, recursive = False).strip()\n",
    "    \n",
    "    # document description\n",
    "    document_description = filing_document.description.find(text = True, recursive = False).strip()\n",
    "    \n",
    "    #insert the key\n",
    "    master_document_dict[document_id] = {}\n",
    "    \n",
    "    # add different parts of the document\n",
    "    master_document_dict[document_id]['document_sequence'] = document_sequence\n",
    "    master_document_dict[document_id]['document_filename'] = document_filename\n",
    "    master_document_dict[document_id]['document_description'] = document_description \n",
    "\n",
    "## check it out:    \n",
    "#master_document_dict\n",
    "## and recomment please, thank you!\n",
    "\n",
    "    # add the document itself\n",
    "    master_document_dict[document_id]['document_code'] = filing_document.extract()\n",
    "    \n",
    "    # get all text in document\n",
    "    filing_doc_text = filing_document.find('text').extract()\n",
    "    \n",
    "    # get all the thematic breaks\n",
    "    all_thematic_breaks = filing_doc_text.find_all('hr',{'width':'100%'})\n",
    "    \n",
    "    # convert all the breaks into a string vist list comprehension\n",
    "    all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    #prep document for being split\n",
    "    filing_doc_string = str(filing_doc_text)\n",
    "    \n",
    "    if len(all_thematic_breaks) > 0:\n",
    "        # creates our string pattern\n",
    "        regex_delimited_pattern = '|'.join(map(re.escape,all_thematic_breaks))\n",
    "        #split the document along thematic breaks\n",
    "        split_filing_string = re.split(regex_delimited_pattern, filing_doc_string)\n",
    "        #store document in dictionary\n",
    "        master_document_dict[document_id]['pages_code'] = split_filing_string\n",
    "        \n",
    "    elif len(all_thematic_breaks) == 0:\n",
    "        #store document inside a dictionary\n",
    "        master_document_dict[document_id]['pages_code'] = [filing_doc_string]\n",
    "        \n",
    "    #display info to the user\n",
    "    print('-'*80)\n",
    "    print('The document {} was parsed'.format(document_id))\n",
    "    print('There was {} thematic break(s) found.'.format(len(all_thematic_breaks)))\n",
    "    \n",
    "# store document in the master filing dictionary\n",
    "master_filings_dict[accession_number]['filing_documents'] = master_document_dict\n",
    "\n",
    "print('-'*80)\n",
    "print('All the documents for filing {} were parsed and stored.'.format(accession_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first grab the documents\n",
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "#loop through each document\n",
    "for document_id in filing_documents:\n",
    "    #grab all documents\n",
    "    document_pages = filing_documents[document_id]['pages_code']\n",
    "    #page length\n",
    "    pages_length = len(document_pages)\n",
    "    #initialize some dictionaries\n",
    "    repaired_pages = {}\n",
    "    normalized_text = {}\n",
    "    \n",
    "    for index, page in enumerate(document_pages):\n",
    "        #pass through parser to repair it\n",
    "        page_soup = BeautifulSoup(page, 'html5')\n",
    "        #grab test from each page\n",
    "        page_text = page_soup.html.body.get_text(' ', strip = True)\n",
    "        #normalize the text\n",
    "        page_text_norm = restore_windows_1252_characters(unicodedata.normalize('NFKD', page_text))\n",
    "        page_text_norm = page_text_norm.replace('  ',' ').replace('\\n',' ')\n",
    "        \n",
    "        ## check it out\n",
    "        #print(page_text_norm)\n",
    "        ## and please recomment, thank you!\n",
    "        \n",
    "        #define our page number\n",
    "        page_number = index + 1\n",
    "        #add normalized text to dictionary\n",
    "        normalized_text[page_number] = page_text_norm\n",
    "        #add repaired html code to dictionary\n",
    "        repaired_pages[page_number] = page_soup\n",
    "        # add normalized text dict to master filing dictionary\n",
    "        filing_documents[document_id]['page_normalized_text'] = normalized_text\n",
    "        filing_documents[document_id]['pages_code'] = repaired_pages\n",
    "        \n",
    "        # add page numbers we generate\n",
    "        gen_page_numbers = list(repaired_pages.keys())\n",
    "        \n",
    "        # add normalized text dictionary to the master filing dictionary\n",
    "        filing_documents[document_id]['pages_numbers_generated'] = gen_page_numbers\n",
    "        \n",
    "        # display a status to the user.\n",
    "        print('All the pages from document {} have been normalized.'.format(document_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "# master_filings_dict[accession_number]['filing_documents']['EX-10.1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining search words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dict = {\n",
    "    \n",
    "    # these could possibly be words that help us find pages that discuss financial statements.\n",
    "    'financial_words':['liability', 'asset'],\n",
    "    \n",
    "    # these could possible be words that help us find sections that discuss administration topics.\n",
    "    'admin_words':['administration', 'government']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not my code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    \n",
    "    ####################################\n",
    "    # THIS WILL HANDLE THE WORD SEARCH #\n",
    "    ####################################\n",
    "    \n",
    "    \n",
    "    # let's grab the normalized text in this example, since it's cleaned and easier to search\n",
    "    normalized_text_dict = filing_documents[document_id]['page_normalized_text']  \n",
    "            \n",
    "    # initalize a dictionary to store all the tables we find.\n",
    "    matching_words_dict = {}\n",
    "    \n",
    "    # define the number of pages\n",
    "    page_length = len(normalized_text_dict)\n",
    "    \n",
    "    # loop through all the text\n",
    "    for page_num in normalized_text_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        normalized_page_text = normalized_text_dict[page_num]\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house each pages result.\n",
    "        matching_words_dict[page_num] = {}\n",
    "        \n",
    "        # loop through each word list in the search dictionary.\n",
    "        for search_list in search_dict:\n",
    "            \n",
    "            # grab the list of words.\n",
    "            list_of_words = search_dict[search_list]\n",
    "            \n",
    "            # lets see if any of the words are found\n",
    "            matching_words = [word for word in list_of_words if word in normalized_page_text]\n",
    "            \n",
    "            '''\n",
    "                Again, I know list comprehension might be hard to understand so I'll show you what the loop\n",
    "                looks like.\n",
    "                \n",
    "                # initalize a list of matching words.\n",
    "                matching_words = []\n",
    "                \n",
    "                # loop through the list of words.\n",
    "                for word in list_of_words:\n",
    "                \n",
    "                    # check to see if it's in the text\n",
    "                    if word in normalized_page_text:\n",
    "                        \n",
    "                        # if it is then add it to the list.\n",
    "                        matching_words.append(word)\n",
    "            '''\n",
    "            \n",
    "            # each page will have a set of results, list of words\n",
    "            matching_words_dict[page_num][search_list] = {}\n",
    "            \n",
    "            # let's add the list of words we search to the matching words dictionary first.\n",
    "            matching_words_dict[page_num][search_list]['list_of_words'] = list_of_words\n",
    "            \n",
    "            # next let's add the list of matchings words to the matching words dictionary.\n",
    "            matching_words_dict[page_num][search_list]['matches'] = matching_words\n",
    "            \n",
    "        \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} has been searched.'.format(page_num, page_length, document_id))\n",
    "    \n",
    "    \n",
    "    # display a status to the user.\n",
    "    print('-'*80)    \n",
    "    print('All the pages from document {} have been searched.'.format(document_id))    \n",
    "    \n",
    "    \n",
    "    ####################################\n",
    "    # THIS WILL HANDLE THE LINK SEARCH #\n",
    "    ####################################\n",
    "    \n",
    "    \n",
    "    # let's grab the all pages code.\n",
    "    pages_dict = filing_documents[document_id]['pages_code']  \n",
    "            \n",
    "    # initalize a dictionary to store all the anchors we find.\n",
    "    link_anchor_dict = {}\n",
    "    \n",
    "    # loop through each page\n",
    "    for page_num in pages_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        page_code = pages_dict[page_num]\n",
    "        \n",
    "        # find all the anchors in the page, that have the attribute 'name'\n",
    "        anchors_found = page_code.find_all('a',{'name':True})\n",
    "        \n",
    "        # number of anchors found\n",
    "        num_found = len(anchors_found)\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house all the anchors found.\n",
    "        link_anchor_dict[page_num]= {(anchor_id + 1): anchor for anchor_id, anchor in enumerate(anchors_found)}        \n",
    "    \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} contained {} anchors with names.'.format(page_num, \n",
    "                                                                                       page_length, \n",
    "                                                                                       document_id, \n",
    "                                                                                       num_found))\n",
    "    \n",
    "    # display a status to the user.  \n",
    "    print('All the pages from document {} have been scraped for anchors with names.'.format(document_id)) \n",
    "    print('-'*80)  \n",
    "    \n",
    "    \n",
    "    #####################################\n",
    "    # THIS WILL HANDLE THE TABLE SEARCH #\n",
    "    #####################################\n",
    "    \n",
    "         \n",
    "    # let's grab the all pages code.\n",
    "    pages_dict = filing_documents[document_id]['pages_code']  \n",
    "            \n",
    "    # initalize a dictionary to store matching words.\n",
    "    tables_dict = {}\n",
    "    \n",
    "    # loop through each page\n",
    "    for page_num in pages_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        page_code = pages_dict[page_num]\n",
    "        \n",
    "        # find all the tables\n",
    "        tables_found = page_code.find_all('table')\n",
    "        \n",
    "        # number of tables found\n",
    "        num_found = len(tables_found)\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house all the tables found.\n",
    "        tables_dict[page_num] = {(table_id + 1): table for table_id, table in enumerate(tables_found)}        \n",
    "    \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} contained {} tables.'.format(page_num, page_length, document_id, num_found))\n",
    "    \n",
    "    # display a status to the user.  \n",
    "    print('All the pages from document {} have been scraped for tables.'.format(document_id)) \n",
    "    print('-'*80)    \n",
    "    \n",
    "        \n",
    "    # let's add the matching words dict to the document.\n",
    "    filing_documents[document_id]['word_search'] = matching_words_dict  \n",
    "    \n",
    "    # let's add the matching tables dict to the document.\n",
    "    filing_documents[document_id]['table_search'] = tables_dict\n",
    "    \n",
    "    # let's add the matching anchors dict to the document.\n",
    "    filing_documents[document_id]['anchor_search'] = link_anchor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My code gives errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# first grab all documents\n",
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "#loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    #grab normalized text\n",
    "    normalized_text_dict = filing_documents[document_id]['page_normalized_text']   \n",
    "    \n",
    "    #initialize a dictionary of our results\n",
    "    matching_words_dict = {}\n",
    "    \n",
    "    # define number of pages\n",
    "    page_length = len(normalized_text_dict)\n",
    "    #loop through all the text\n",
    "    for page_num in normalized_text_dict:\n",
    "        #grab the actual text\n",
    "        normalized_page_text = normalized_text_dict[page_num]\n",
    "        #each page will have a set of results\n",
    "        matching_words_dict[page_num] = {}\n",
    "        \n",
    "        # loop through each list in our search dictionary\n",
    "        for search_list in search_dict:\n",
    "            #grab list of words\n",
    "            list_of_words = search_dict[search_list]\n",
    "            # do any of the words match?\n",
    "            matching_words = [word for word in list_of_words if word in normalized_page_text]\n",
    "            \n",
    "        # each page will have a set of results, list of words\n",
    "        matching_words_dict[page_num][search_list]['list_of_words'] = list_of_words\n",
    "        \n",
    "        # each page will have a set of results, list of words\n",
    "        matching_words_dict[page_num][search_list]['matches'] = matching_words\n",
    "    \n",
    "    filing_document[document_id]['word_search'] = matching_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_table_dictionary(table_dictionary):\n",
    "    \n",
    "    # initalize a new dicitonary that'll house all your results\n",
    "    new_table_dictionary = {}\n",
    "    \n",
    "    if len(table_dictionary) != 0:\n",
    "\n",
    "        # loop through the dictionary\n",
    "        for table_id in table_dictionary:\n",
    "\n",
    "            # grab the table\n",
    "            table_html = table_dictionary[table_id]\n",
    "            \n",
    "            # grab all the rows.\n",
    "            table_rows = table_html.find_all('tr')\n",
    "            \n",
    "            # parse the table, first loop through the rows, then each element, and then parse each element.\n",
    "            parsed_table = [\n",
    "                [element.get_text(strip=True) for element in row.find_all('td')]\n",
    "                for row in table_rows\n",
    "            ]\n",
    "            \n",
    "            # keep the original just to be safe.\n",
    "            new_table_dictionary[table_id]['original_table'] = table_html\n",
    "            \n",
    "            # add the new parsed table.\n",
    "            new_table_dictionary[table_id]['parsed_table'] = parsed_table\n",
    "            \n",
    "            # here some additional steps you can take to clean up the data - Removing '$'.\n",
    "            parsed_table_cleaned = [\n",
    "                [element for element in row if element != '$']\n",
    "                for row in parsed_table\n",
    "            ]\n",
    "            \n",
    "            # here some additional steps you can take to clean up the data - Removing Blanks.\n",
    "            parsed_table_cleaned = [\n",
    "                [element for element in row if element != None]\n",
    "                for row in parsed_table_cleaned\n",
    "            ]\n",
    "\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # if there are no tables then just have the id equal NONE\n",
    "        new_table_dictionary[1]['original_table'] = None\n",
    "        new_table_dictionary[1]['parsed_table'] = None\n",
    "        \n",
    "    return new_table_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
