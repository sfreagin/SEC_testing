{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing from this site: \n",
    "\n",
    "https://github.com/areed1192/sigma_coding_youtube/blob/master/python/python-finance/sec-web-scraping/Web%20Scraping%20SEC%20-%20Parsing%20SEC%20Documents%20-%20New%20Filings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our libraries\n",
    "import re\n",
    "import requests\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "        Replace C1 control characters in the Unicode string s by the\n",
    "        characters at the corresponding code points in Windows-1252,\n",
    "        where possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "        except UnicodeDecodeError:\n",
    "            # No character at the corresponding code point: remove it.\n",
    "            return ''\n",
    "        \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_html_text = r\"https://www.sec.gov/Archives/edgar/data/1166036/000110465904027382/0001104659-04-027382.txt\"\n",
    "\n",
    "# grab the response\n",
    "response = requests.get(new_html_text)\n",
    "\n",
    "# pass it through the parser, in this case let's just use lxml because the tags seem to follow xml.\n",
    "soup = BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary that will house all filings.\n",
    "master_filings_dict = {}\n",
    "\n",
    "# let's use the accession number as the key. This \n",
    "accession_number = '0001104659-04-027382'\n",
    "\n",
    "# add a new level to our master_filing_dict, this will also be a dictionary.\n",
    "master_filings_dict[accession_number] = {}\n",
    "\n",
    "# this dictionary will contain two keys, the sec header content, and a documents key.\n",
    "master_filings_dict[accession_number]['sec_header_content'] = {}\n",
    "master_filings_dict[accession_number]['filing_documents'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sec-header>0001104659-04-027382.hdr.sgml : 20040913\n",
       "<acceptance-datetime>20040913074905\n",
       "ACCESSION NUMBER:\t\t0001104659-04-027382\n",
       "CONFORMED SUBMISSION TYPE:\t8-K/A\n",
       "PUBLIC DOCUMENT COUNT:\t\t7\n",
       "CONFORMED PERIOD OF REPORT:\t20040730\n",
       "ITEM INFORMATION:\t\tCompletion of Acquisition or Disposition of Assets\n",
       "ITEM INFORMATION:\t\tFinancial Statements and Exhibits\n",
       "FILED AS OF DATE:\t\t20040913\n",
       "DATE AS OF CHANGE:\t\t20040913\n",
       "\n",
       "FILER:\n",
       "\n",
       "\tCOMPANY DATA:\t\n",
       "\t\tCOMPANY CONFORMED NAME:\t\t\tMARKWEST ENERGY PARTNERS L P\n",
       "\t\tCENTRAL INDEX KEY:\t\t\t0001166036\n",
       "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tCRUDE PETROLEUM &amp; NATURAL GAS [1311]\n",
       "\t\tIRS NUMBER:\t\t\t\t270005456\n",
       "\t\tFISCAL YEAR END:\t\t\t1231\n",
       "\n",
       "\tFILING VALUES:\n",
       "\t\tFORM TYPE:\t\t8-K/A\n",
       "\t\tSEC ACT:\t\t1934 Act\n",
       "\t\tSEC FILE NUMBER:\t001-31239\n",
       "\t\tFILM NUMBER:\t\t041026639\n",
       "\n",
       "\tBUSINESS ADDRESS:\t\n",
       "\t\tSTREET 1:\t\t155 INVERNESS DR WEST\n",
       "\t\tSTREET 2:\t\tSTE 200\n",
       "\t\tCITY:\t\t\tENGLEWOOD\n",
       "\t\tSTATE:\t\t\tCO\n",
       "\t\tZIP:\t\t\t80112\n",
       "\t\tBUSINESS PHONE:\t\t303-925-9275\n",
       "\n",
       "\tMAIL ADDRESS:\t\n",
       "\t\tSTREET 1:\t\t155 INVERNESS DR WEST\n",
       "\t\tSTREET 2:\t\tSTE 200\n",
       "\t\tCITY:\t\t\tENGLEWOOD\n",
       "\t\tSTATE:\t\t\tCO\n",
       "\t\tZIP:\t\t\t80112\n",
       "</acceptance-datetime></sec-header>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grab the sec-header tag, so we can store it in the master filing dictionary.\n",
    "sec_header_tag = soup.find('sec-header')\n",
    "\n",
    "# store the tag in the dictionary just as is.\n",
    "master_filings_dict[accession_number]['sec_header_content']['sec_header_code'] = sec_header_tag\n",
    "\n",
    "# display the sec header tag, so you can see how it looks.\n",
    "display(sec_header_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "The document EX-2.1 was parsed.\n",
      "There was 37 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-4.1 was parsed.\n",
      "There was 35 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-4.2 was parsed.\n",
      "There was 20 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-23.1 was parsed.\n",
      "There was 0 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-99.1 was parsed.\n",
      "There was 235 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "The document EX-99.2 was parsed.\n",
      "There was 13 thematic breaks(s) found.\n",
      "--------------------------------------------------------------------------------\n",
      "All the documents for filing 0001104659-04-027382 were parsed and stored.\n"
     ]
    }
   ],
   "source": [
    "master_document_dict = {}\n",
    "\n",
    "# find all the documents in the filing.\n",
    "for filing_document in soup.find_all('document'):\n",
    "    \n",
    "    # define the document type, found under the <type> tag, this will serve as our key for the dictionary.\n",
    "    document_id = filing_document.type.find(text=True, recursive=False).strip()\n",
    "    \n",
    "    # here are the other parts if you want them.\n",
    "    document_sequence = filing_document.sequence.find(text=True, recursive=False).strip()\n",
    "    document_filename = filing_document.filename.find(text=True, recursive=False).strip()\n",
    "    document_description = filing_document.description.find(text=True, recursive=False).strip()\n",
    "    \n",
    "    # initalize our document dictionary\n",
    "    master_document_dict[document_id] = {}\n",
    "    \n",
    "    # add the different parts, we parsed up above.\n",
    "    master_document_dict[document_id]['document_sequence'] = document_sequence\n",
    "    master_document_dict[document_id]['document_filename'] = document_filename\n",
    "    master_document_dict[document_id]['document_description'] = document_description\n",
    "    \n",
    "    # store the document itself, this portion extracts the HTML code. We will have to reparse it later.\n",
    "    master_document_dict[document_id]['document_code'] = filing_document.extract()\n",
    "    \n",
    "    \n",
    "    # grab the text portion of the document, this will be used to split the document into pages.\n",
    "    filing_doc_text = filing_document.find('text').extract()\n",
    "\n",
    "    \n",
    "    # find all the thematic breaks, these help define page numbers and page breaks.\n",
    "    all_thematic_breaks = filing_doc_text.find_all('hr',{'width':'100%'})\n",
    "    \n",
    "    \n",
    "    ###########################\n",
    "    ### Optional part ommitted\n",
    "    ###########################\n",
    "    \n",
    "    # convert all thematic breaks to a string so it can be used for parsing\n",
    "    all_thematic_breaks = [str(thematic_break) for thematic_break in all_thematic_breaks]\n",
    "    \n",
    "    # prep the document text for splitting, this means converting it to a string.\n",
    "    filing_doc_string = str(filing_doc_text)\n",
    "\n",
    "    \n",
    "    # handle the case where there are thematic breaks.\n",
    "    if len(all_thematic_breaks) > 0:\n",
    "    \n",
    "        # define the regex delimiter pattern, this would just be all of our thematic breaks.\n",
    "        regex_delimiter_pattern = '|'.join(map(re.escape, all_thematic_breaks))\n",
    "\n",
    "        # split the document along each thematic break.\n",
    "        split_filing_string = re.split(regex_delimiter_pattern, filing_doc_string)\n",
    "\n",
    "        # store the document itself\n",
    "        master_document_dict[document_id]['pages_code'] = split_filing_string\n",
    "\n",
    "    # handle the case where there are no thematic breaks.\n",
    "    elif len(all_thematic_breaks) == 0:\n",
    "\n",
    "        # handles so it will display correctly.\n",
    "        split_filing_string = all_thematic_breaks\n",
    "        \n",
    "        # store the document as is, since there are no thematic breaks. In other words, no splitting.\n",
    "        master_document_dict[document_id]['pages_code'] = [filing_doc_string]\n",
    "    \n",
    "\n",
    "    # display some information to the user.\n",
    "    print('-'*80)\n",
    "    print('The document {} was parsed.'.format(document_id))\n",
    "    print('There was {} thematic breaks(s) found.'.format(len(all_thematic_breaks)))\n",
    "    \n",
    "\n",
    "# store the documents in the master_filing_dictionary.\n",
    "master_filings_dict[accession_number]['filing_documents'] = master_document_dict\n",
    "\n",
    "print('-'*80)\n",
    "print('All the documents for filing {} were parsed and stored.'.format(accession_number))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Pulling document EX-2.1 for text normilzation.\n",
      "All the pages from document EX-2.1 have been normalized.\n",
      "--------------------------------------------------------------------------------\n",
      "Pulling document EX-4.1 for text normilzation.\n",
      "All the pages from document EX-4.1 have been normalized.\n",
      "--------------------------------------------------------------------------------\n",
      "Pulling document EX-4.2 for text normilzation.\n",
      "All the pages from document EX-4.2 have been normalized.\n",
      "--------------------------------------------------------------------------------\n",
      "Pulling document EX-23.1 for text normilzation.\n",
      "All the pages from document EX-23.1 have been normalized.\n",
      "--------------------------------------------------------------------------------\n",
      "Pulling document EX-99.1 for text normilzation.\n",
      "All the pages from document EX-99.1 have been normalized.\n",
      "--------------------------------------------------------------------------------\n",
      "Pulling document EX-99.2 for text normilzation.\n",
      "All the pages from document EX-99.2 have been normalized.\n"
     ]
    }
   ],
   "source": [
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    # display some info to give status updates.\n",
    "    print('-'*80)\n",
    "    print('Pulling document {} for text normilzation.'.format(document_id))\n",
    "    \n",
    "    # grab all the pages for that document\n",
    "    document_pages = filing_documents[document_id]['pages_code']\n",
    "    \n",
    "    # page length\n",
    "    pages_length = len(filing_documents[document_id]['pages_code'])\n",
    "    \n",
    "    # initalize a dictionary that'll house our repaired html code for each page.\n",
    "    repaired_pages = {}\n",
    "    \n",
    "    # initalize a dictionary that'll house all the normalized text.\n",
    "    normalized_text = {}\n",
    "\n",
    "    # loop through each page in that document.\n",
    "    for index, page in enumerate(document_pages):\n",
    "        \n",
    "        # pass it through the parser. NOTE I AM USING THE HTML5 PARSER. YOU MUST USE THIS TO FIX BROKEN TAGS.\n",
    "        page_soup = BeautifulSoup(page,'html5')\n",
    "        \n",
    "        # grab all the text, notice I go to the BODY tag to do this\n",
    "        page_text = page_soup.html.body.get_text(' ',strip = True)\n",
    "        \n",
    "        # normalize the text, remove messy characters. Additionally, restore missing window characters.\n",
    "        page_text_norm = restore_windows_1252_characters(unicodedata.normalize('NFKD', page_text)) \n",
    "        \n",
    "        # Additional cleaning steps, removing double spaces, and new line breaks.\n",
    "        page_text_norm = page_text_norm.replace('  ', ' ').replace('\\n',' ')\n",
    "                \n",
    "        #########################\n",
    "        ## Optional part ommitted\n",
    "        #########################\n",
    "        \n",
    "    # add the normalized text back to the document dictionary\n",
    "    filing_documents[document_id]['pages_normalized_text'] = normalized_text\n",
    "    \n",
    "    # add the repaired html code back to the document dictionary\n",
    "    filing_documents[document_id]['pages_code'] = repaired_pages\n",
    "    \n",
    "    # define the generated page numbers\n",
    "    gen_page_numbers = list(repaired_pages.keys())\n",
    "    \n",
    "    # add the page numbers we have.\n",
    "    filing_documents[document_id]['pages_numbers_generated'] = gen_page_numbers    \n",
    "    \n",
    "    # display a status to the user.\n",
    "    print('All the pages from document {} have been normalized.'.format(document_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dict = {\n",
    "    \n",
    "    # these could possibly be words that help us find pages that discuss financial statements.\n",
    "    'financial_words':['liability', 'asset'],\n",
    "    \n",
    "    # these could possible be words that help us find sections that discuss administration topics.\n",
    "    'admin_words':['administration', 'government']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-2.1 have been searched.\n",
      "All the pages from document EX-2.1 have been scraped for anchors with names.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-2.1 have been scraped for tables.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-4.1 have been searched.\n",
      "All the pages from document EX-4.1 have been scraped for anchors with names.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-4.1 have been scraped for tables.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-4.2 have been searched.\n",
      "All the pages from document EX-4.2 have been scraped for anchors with names.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-4.2 have been scraped for tables.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-23.1 have been searched.\n",
      "All the pages from document EX-23.1 have been scraped for anchors with names.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-23.1 have been scraped for tables.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-99.1 have been searched.\n",
      "All the pages from document EX-99.1 have been scraped for anchors with names.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-99.1 have been scraped for tables.\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-99.2 have been searched.\n",
      "All the pages from document EX-99.2 have been scraped for anchors with names.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-99.2 have been scraped for tables.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# first grab all the documents\n",
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:\n",
    "    \n",
    "    \n",
    "    ####################################\n",
    "    # THIS WILL HANDLE THE WORD SEARCH #\n",
    "    ####################################\n",
    "    \n",
    "    \n",
    "    # let's grab the normalized text in this example, since it's cleaned and easier to search\n",
    "    normalized_text_dict = filing_documents[document_id]['pages_normalized_text']  \n",
    "            \n",
    "    # initalize a dictionary to store all the tables we find.\n",
    "    matching_words_dict = {}\n",
    "    \n",
    "    # define the number of pages\n",
    "    page_length = len(normalized_text_dict)\n",
    "    \n",
    "    # loop through all the text\n",
    "    for page_num in normalized_text_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        normalized_page_text = normalized_text_dict[page_num]\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house each pages result.\n",
    "        matching_words_dict[page_num] = {}\n",
    "        \n",
    "        # loop through each word list in the search dictionary.\n",
    "        for search_list in search_dict:\n",
    "            \n",
    "            # grab the list of words.\n",
    "            list_of_words = search_dict[search_list]\n",
    "            \n",
    "            # lets see if any of the words are found\n",
    "            matching_words = [word for word in list_of_words if word in normalized_page_text]\n",
    "            \n",
    "            '''\n",
    "                Again, I know list comprehension might be hard to understand so I'll show you what the loop\n",
    "                looks like.\n",
    "                \n",
    "                # initalize a list of matching words.\n",
    "                matching_words = []\n",
    "                \n",
    "                # loop through the list of words.\n",
    "                for word in list_of_words:\n",
    "                \n",
    "                    # check to see if it's in the text\n",
    "                    if word in normalized_page_text:\n",
    "                        \n",
    "                        # if it is then add it to the list.\n",
    "                        matching_words.append(word)\n",
    "            '''\n",
    "            \n",
    "            # each page will have a set of results, list of words\n",
    "            matching_words_dict[page_num][search_list] = {}\n",
    "            \n",
    "            # let's add the list of words we search to the matching words dictionary first.\n",
    "            matching_words_dict[page_num][search_list]['list_of_words'] = list_of_words\n",
    "            \n",
    "            # next let's add the list of matchings words to the matching words dictionary.\n",
    "            matching_words_dict[page_num][search_list]['matches'] = matching_words\n",
    "            \n",
    "        \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} has been searched.'.format(page_num, page_length, document_id))\n",
    "    \n",
    "    \n",
    "    # display a status to the user.\n",
    "    print('-'*80)    \n",
    "    print('All the pages from document {} have been searched.'.format(document_id))    \n",
    "    \n",
    "    \n",
    "    ####################################\n",
    "    # THIS WILL HANDLE THE LINK SEARCH #\n",
    "    ####################################\n",
    "    \n",
    "    \n",
    "    # let's grab the all pages code.\n",
    "    pages_dict = filing_documents[document_id]['pages_code']  \n",
    "            \n",
    "    # initalize a dictionary to store all the anchors we find.\n",
    "    link_anchor_dict = {}\n",
    "    \n",
    "    # loop through each page\n",
    "    for page_num in pages_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        page_code = pages_dict[page_num]\n",
    "        \n",
    "        # find all the anchors in the page, that have the attribute 'name'\n",
    "        anchors_found = page_code.find_all('a',{'name':True})\n",
    "        \n",
    "        # number of anchors found\n",
    "        num_found = len(anchors_found)\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house all the anchors found.\n",
    "        link_anchor_dict[page_num]= {(anchor_id + 1): anchor for anchor_id, anchor in enumerate(anchors_found)}        \n",
    "    \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} contained {} anchors with names.'.format(page_num, \n",
    "                                                                                       page_length, \n",
    "                                                                                       document_id, \n",
    "                                                                                       num_found))\n",
    "    \n",
    "    # display a status to the user.  \n",
    "    print('All the pages from document {} have been scraped for anchors with names.'.format(document_id)) \n",
    "    print('-'*80)  \n",
    "    \n",
    "    \n",
    "    #####################################\n",
    "    # THIS WILL HANDLE THE TABLE SEARCH #\n",
    "    #####################################\n",
    "    \n",
    "         \n",
    "    # let's grab the all pages code.\n",
    "    pages_dict = filing_documents[document_id]['pages_code']  \n",
    "            \n",
    "    # initalize a dictionary to store matching words.\n",
    "    tables_dict = {}\n",
    "    \n",
    "    # loop through each page\n",
    "    for page_num in pages_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        page_code = pages_dict[page_num]\n",
    "        \n",
    "        # find all the tables\n",
    "        tables_found = page_code.find_all('table')\n",
    "        \n",
    "        # number of tables found\n",
    "        num_found = len(tables_found)\n",
    "        \n",
    "        # each page is going to be checked, so let's have another dictionary that'll house all the tables found.\n",
    "        tables_dict[page_num] = {(table_id + 1): table for table_id, table in enumerate(tables_found)}        \n",
    "    \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} contained {} tables.'.format(page_num, page_length, document_id, num_found))\n",
    "    \n",
    "    # display a status to the user.  \n",
    "    print('All the pages from document {} have been scraped for tables.'.format(document_id)) \n",
    "    print('-'*80)    \n",
    "    \n",
    "        \n",
    "    # let's add the matching words dict to the document.\n",
    "    filing_documents[document_id]['word_search'] = matching_words_dict  \n",
    "    \n",
    "    # let's add the matching tables dict to the document.\n",
    "    filing_documents[document_id]['table_search'] = tables_dict\n",
    "    \n",
    "    # let's add the matching anchors dict to the document.\n",
    "    filing_documents[document_id]['anchor_search'] = link_anchor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_table_dictionary(table_dictionary):\n",
    "    \n",
    "    # initalize a new dicitonary that'll house all your results\n",
    "    new_table_dictionary = {}\n",
    "    \n",
    "    if len(table_dictionary) != 0:\n",
    "\n",
    "        # loop through the dictionary\n",
    "        for table_id in table_dictionary:\n",
    "\n",
    "            # grab the table\n",
    "            table_html = table_dictionary[table_id]\n",
    "            \n",
    "            # grab all the rows.\n",
    "            table_rows = table_html.find_all('tr')\n",
    "            \n",
    "            # parse the table, first loop through the rows, then each element, and then parse each element.\n",
    "            parsed_table = [\n",
    "                [element.get_text(strip=True) for element in row.find_all('td')]\n",
    "                for row in table_rows\n",
    "            ]\n",
    "            \n",
    "            # keep the original just to be safe.\n",
    "            new_table_dictionary[table_id]['original_table'] = table_html\n",
    "            \n",
    "            # add the new parsed table.\n",
    "            new_table_dictionary[table_id]['parsed_table'] = parsed_table\n",
    "            \n",
    "            # here some additional steps you can take to clean up the data - Removing '$'.\n",
    "            parsed_table_cleaned = [\n",
    "                [element for element in row if element != '$']\n",
    "                for row in parsed_table\n",
    "            ]\n",
    "            \n",
    "            # here some additional steps you can take to clean up the data - Removing Blanks.\n",
    "            parsed_table_cleaned = [\n",
    "                [element for element in row if element != None]\n",
    "                for row in parsed_table_cleaned\n",
    "            ]\n",
    "\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # if there are no tables then just have the id equal NONE\n",
    "        new_table_dictionary[1]['original_table'] = None\n",
    "        new_table_dictionary[1]['parsed_table'] = None\n",
    "        \n",
    "    return new_table_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_centered_headers(tag):\n",
    "\n",
    "    # easy way to end early is check if the 'align' keet is in attributes.\n",
    "    if 'align' not in tag.attrs:\n",
    "        return\n",
    "    \n",
    "    # define the criteria.\n",
    "    criteria1 = tag.name == 'p'                # I want the tag to be name of 'p'\n",
    "    criteria2 = tag.parent.name != 'td'        # I want the parent tag NOT to be named 'td'\n",
    "    criteria3 = tag['align'] == 'center'       # I want the 'align' attribute to be labeled 'center'.\n",
    "    \n",
    "    # if it matches all the criteria then return the text.\n",
    "    if criteria1 and criteria2 and criteria3:         \n",
    "        return tag.get_text(strip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_bolded_tags(tag):\n",
    "    \n",
    "    # define the criteria.\n",
    "    criteria1 = tag.name == 'b'                # I want the tag to be name of 'p'\n",
    "    criteria2 = tag.parent.name != 'td'        # I want the parent tag NOT to be named 'td'\n",
    "    \n",
    "    # if it matches all the criteria then return the text.\n",
    "    if criteria1 and criteria2:         \n",
    "        return tag.get_text(strip = True).replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the pages from document EX-2.1 have been scraped for centered headers.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-4.1 have been scraped for centered headers.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-4.2 have been scraped for centered headers.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-23.1 have been scraped for centered headers.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-99.1 have been scraped for centered headers.\n",
      "--------------------------------------------------------------------------------\n",
      "All the pages from document EX-99.2 have been scraped for centered headers.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "filing_documents = master_filings_dict[accession_number]['filing_documents']\n",
    "\n",
    "# loop through each document\n",
    "for document_id in filing_documents:   \n",
    "    \n",
    "    # let's grab the all pages code.\n",
    "    pages_dict = filing_documents[document_id]['pages_code']  \n",
    "            \n",
    "    # initalize a dictionary to store all the anchors we find.\n",
    "    centered_headers_dict = {}\n",
    "    \n",
    "    # loop through each page\n",
    "    for page_num in pages_dict:\n",
    "        \n",
    "        # grab the actual text\n",
    "        page_code = pages_dict[page_num]\n",
    "        \n",
    "        # find all the anchors in the page, that have the attribute 'name'\n",
    "        centered_headers_found = page_code.find_all(search_for_centered_headers)\n",
    "        \n",
    "        # number of anchors found\n",
    "        num_found = len(centered_headers_found)\n",
    "   \n",
    "        # display a status to the user.\n",
    "        print('Page {} of {} from document {} contained {} centered headers.'.format(page_num, \n",
    "                                                                                     page_length, \n",
    "                                                                                     document_id, \n",
    "                                                                                     num_found))\n",
    "    \n",
    "    # display a status to the user.  \n",
    "    print('All the pages from document {} have been scraped for centered headers.'.format(document_id)) \n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
